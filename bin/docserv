#!/usr/bin/env python3
#
# Copyright (c) 2018 SUSE Linux GmbH
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of version 3 of the GNU General Public License as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, contact SUSE LLC.
#
# To contact SUSE about this file by physical or electronic mail,
# you may find current contact information at www.suse.com

import json
import time
import subprocess
import shlex
import datetime
import os
import sys
import threading
import queue
import socket
import configparser
import logging
import signal
import tempfile
import string
import random
import hashlib
from http.server import HTTPServer, BaseHTTPRequestHandler
from socketserver import ThreadingMixIn
from xml.etree import ElementTree, cElementTree
my_env = os.environ


def resource_to_filename(url):
    # create something like https_github_com_SUSE_doc_sle
    # from https://github.com/SUSE/doc-sle
    replace = "/\\-.,:;#+`´{}()[]!\"§$%&"
    for char in replace:
        url = str(url).replace(char, '_')
    return url


class RepoLock:
    def __init__(self, repo_dir, thread_id, gitLocks, gitLocksLock):
        self.gitLocks = gitLocks
        self.gitLocksLock = gitLocksLock
        self.resource_name = resource_to_filename(repo_dir)
        self.gitLocksLock.acquire()
        if self.resource_name not in gitLocks:
            self.gitLocks[self.resource_name] = threading.Lock()
        self.gitLocksLock.release()
        self.acquired = False
        self.thread_id = thread_id

    def acquire(self, blocking=True):
        if self.gitLocks[self.resource_name].acquire(blocking):
            self.acquired = True
            logger.debug("Thread %i: Acquired lock %s." % (self.thread_id,
                                                           self.resource_name))
            return True
        return False

    def release(self):
        if self.acquired:
            self.gitLocks[self.resource_name].release()
            self.acquired = False
            logger.debug("Thread %i: Released lock %s." % (self.thread_id,
                                                           self.resource_name))

class Deliverable:
    title = None
    path = []
    status = "building"
    successful_build_commit = None
    last_build_attempt_commit = None
    root_id = None # False if no root id exists

    def __init__(self, parent, dc_file, build_format, subdeliverables):
        self.parent = parent # Reference to the parent BuildInstructionHandler
        self.dc_file = dc_file
        self.build_format = build_format
        self.subdeliverables = subdeliverables
        self.id = self.generate_id()
        self.prev_state()
        logger.debug("Created deliverable %s (%s, %s) for BI %s" % (
                                                                self.id,
                                                                self.dc_file,
                                                                self.build_format,
                                                                self.parent.build_instruction['id']
                                                             ))

    def prev_state(self):
        if self.id in self.parent.deliverables.keys():
            self.successful_build_commit = self.parent.deliverables[self.id]['successful_build_commit']
            self.last_build_attempt_commit = self.parent.deliverables[self.id]['last_build_attempt_commit']

    def dict(self):
        value = {
            'build_format': self.build_format,
            'dc': self.dc_file,
            'status': self.status,
            'title': self.title,
            'path': self.path,
            'successful_build_commit': self.successful_build_commit,
            'last_build_attempt_commit': self.last_build_attempt_commit,
            'subdeliverables': self.subdeliverables,
        }
        return value

    def generate_id(self):
        return hashlib.md5((self.parent.build_instruction['target'] +
                            self.parent.build_instruction['docset'] +
                            self.parent.build_instruction['lang'] +
                            self.parent.build_instruction['product'] +
                            self.dc_file +
                            self.build_format).encode('utf-8')
                          ).hexdigest()[:9]

    def run(self, thread_id):
        if self.parent.build_instruction['commit'] == self.parent.deliverables[self.id]['successful_build_commit']:
            logger.debug("Deliverable %s (%s, %s) for BI %s already up to date. Skipping daps run." % (
                                                                self.id,
                                                                self.dc_file,
                                                                self.build_format,
                                                                self.parent.build_instruction['id']
                                                             ))
            return self.finish(True)
        with self.parent.deliverables_open_lock:
            self.parent.deliverables[self.id]['last_build_attempt_commit'] = self.parent.build_instruction['commit']
        logger.info("Building deliverable %s (%s, %s) for BI %s. Commit: %s" % (
                                                                        self.id,
                                                                        self.dc_file,
                                                                        self.build_format,
                                                                        self.parent.build_instruction['id'],
                                                                        self.parent.deliverables[self.id]['last_build_attempt_commit']
                                                                    ))
        #
        # The following lines of code define all bash commands that
        # are required to build and publish the deliverable.
        # This includes preparation of the target directories and
        # clean up after the build is finished.
        #
        commands = {}

        # Write XSLT parameters to temp file
        n = 0
        xslt_params_file = tempfile.mkstemp(prefix="docserv_xslt_", text=True)
        xslt_params = ""
        commands[n] = {}
        commands[n]['cmd'] = "echo \"%s\" > %s" % (xslt_params,
                                                   xslt_params_file[1])
        commands[n]['ret_val'] = 0

        # Write daps parameters to temp file
        n += 1
        daps_params_file = tempfile.mkstemp(prefix="docserv_daps_", text=True)
        remarks = self.parent.config['targets'][self.parent.build_instruction['target']]['remarks']
        draft = self.parent.config['targets'][self.parent.build_instruction['target']]['draft']
        meta = self.parent.config['targets'][self.parent.build_instruction['target']]['meta']
        daps_params = " ".join([
                                    "--remarks" if (remarks == "true" or remarks == "1") else "",
                                    "--draft" if (draft == "true" or draft == "1") else "",
                                    "--meta" if (meta == "true" or meta == "1") else ""
                                ])
        commands[n] = {}
        commands[n]['cmd'] = "echo \"%s\" > %s" % (daps_params,
                                                   daps_params_file[1])
        commands[n]['ret_val'] = 0

        # Run daps in the docker container, copy results to a
        # build target directory
        n += 1
        tmp_build_target = tempfile.mkdtemp(prefix="docserv_out_")
        commands[n] = {}
        commands[n]['cmd'] = "/usr/bin/d2d_runner -b=1 -v=1 -u=1 -x=%s -d=%s -o=%s -i=%s -f=%s %s" % (
                                                xslt_params_file[1],       # -x
                                                daps_params_file[1],       # -d
                                                tmp_build_target,          # -o
                                                self.parent.local_repo_build_dir, # -i
                                                self.build_format,         # -f
                                                self.dc_file               # last param
                                                )
        commands[n]['ret_val'] = 0

        # Create correct directory structure
        n += 1
        sync_source_tmp = tempfile.mkdtemp(prefix="docserv_sync_")
        if self.build_format == 'html' or self.build_format == 'single-html':
            self.deliverable_relative_path = "%s/%s/%s/%s/%s/" % (
                self.parent.build_instruction['lang'],
                self.parent.build_instruction['product'],
                self.parent.build_instruction['docset'],
                self.build_format,
                self.dc_file.replace('DC-', '')
            )
        else:
            self.deliverable_relative_path = "%s/%s/%s/" % (
                self.parent.build_instruction['lang'],
                self.parent.build_instruction['product'],
                self.parent.build_instruction['docset'],
            )
        sync_source_dir = os.path.join(
            sync_source_tmp,
            self.deliverable_relative_path)
        commands[n] = {}
        commands[n]['cmd'] = "/usr/bin/mkdir -p %s" % (sync_source_dir)
        commands[n]['ret_val'] = 0

        # Copy wanted files to sync source
        n += 1
        commands[n] = {}
        commands[n]['cmd'] = "rsync -r __FILELIST__  %s" % (sync_source_dir)
        commands[n]['ret_val'] = 0
        commands[n]['pre_cmd_hook'] = 'parse_d2d_filelist'
        commands[n]['tmp_build_target'] = tmp_build_target

        # create directory for Deliverable cache file
        deliverable_cache_base_dir =  '/var/cache/docserv/%s' % self.parent.config['server']['name']
        self.deliverable_cache_dir = os.path.join(
                                        deliverable_cache_base_dir,
                                        self.parent.build_instruction['lang'],
                                        self.parent.build_instruction['product'],
                                        self.parent.build_instruction['docset'],
                                        self.build_format,
                                      )
        n += 1
        commands[n] = {}
        commands[n]['cmd'] = "mkdir -p %s" % self.deliverable_cache_dir
        commands[n]['ret_val'] = 0
        commands[n]['tmp_build_target'] = tmp_build_target
        commands[n]['pre_cmd_hook'] = 'extract_root_id' # get root id from bigfile

        # (re-)generate overview page
        n += 1
        commands[n] = {}
        commands[n]['cmd'] = "docserv-buildoverview %s--stitched_config=\"%s\" --ui-languages=\"%s\" --default-ui-language=%s --cache-dir=%s --doc-language=%s --template_dir=%s --output_dir=%s" % (
            "--internal-mode " if self.parent.config['targets'][self.parent.build_instruction['target']]['languages'] == "yes" else "",
            os.path.join(self.parent.stitch_tmp_dir, "docserv_config_full.xml"),
            self.parent.config['targets'][self.parent.build_instruction['target']]['languages'],
            self.parent.config['targets'][self.parent.build_instruction['target']]['default_lang'],
            deliverable_cache_base_dir,
            self.parent.build_instruction['lang'],
            self.parent.config['targets'][self.parent.build_instruction['target']]['template_dir'],
            sync_source_tmp)
        commands[n]['ret_val'] = 0
        commands[n]['pre_cmd_hook'] = 'write_deliverable_cache' # write configuration for overview page

        # rsync build target directory to backup path
        backup_path = self.parent.config['targets'][self.parent.build_instruction['target']]['backup_path']
        n += 1
        commands[n] = {}
        commands[n]['cmd'] = "rsync -lr %s/ %s" % (sync_source_tmp, backup_path)
        commands[n]['ret_val'] = 0

        # rsync built target directory with web server
        target_path = self.parent.config['targets'][self.parent.build_instruction['target']]['target_path']
        n += 1
        commands[n] = {}
        commands[n]['cmd'] = "rsync -lr %s/ %s" % (sync_source_tmp, target_path)
        commands[n]['ret_val'] = 0

        # remove daps parameter file
        n += 1
        commands[n] = {}
        commands[n]['cmd'] = "rm %s" % (daps_params_file[1])
        commands[n]['ret_val'] = 0

        # remove xslt parameter file
        n += 1
        commands[n] = {}
        commands[n]['cmd'] = "rm %s" % (xslt_params_file[1])
        commands[n]['ret_val'] = 0

        # sync source directory
        commands[n] = {}
        commands[n]['cmd'] = "rm -rf %s" % (sync_source_tmp)
        commands[n]['ret_val'] = 0

        # build target directory
        n += 1
        commands[n] = {}
        commands[n]['cmd'] = "rm -rf %s" % (tmp_build_target)
        commands[n]['ret_val'] = 0

        #
        # Now iterate through all commands and execute them
        #
        for i in range(0,n + 1):
            if 'pre_cmd_hook' in commands[i]:
                commands[i] = getattr(self, commands[i]['pre_cmd_hook'])(commands[i], thread_id)
                if commands[i] == False:
                    return self.finish(False)

            result = self.execute(commands[i], thread_id)
            if not result: # abort if one command failed
                return self.finish(False)

            if 'post_cmd_hook' in commands[i]:
                if not getattr(self, commands[i]['post_cmd_hook'])(commands[i], thread_id):
                    return self.finish(False)

        return self.finish(result)

    def execute(self, command, thread_id):
        cmd = shlex.split(command['cmd'])
        logger.debug("Thread %i: %s" % (thread_id, cmd))
        s = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        self.out, self.err = s.communicate()

        if command['ret_val'] is not None and not command['ret_val'] == int(s.returncode):
            logger.warning("Thread %i: Build failed! Unexpected return value %i for '%s'" % (
                thread_id, s.returncode, command['cmd']))
            logger.warning("Thread %i STDOUT: %s" % (thread_id, self.out.decode('utf-8')))
            logger.warning("Thread %i STDERR: %s" % (thread_id, self.err.decode('utf-8')))
            return False
        return True

    def finish(self, result):
        with self.parent.deliverables_open_lock:
            if result:
                self.parent.deliverables[self.id]['status'] = "success"
            else:
                self.parent.deliverables[self.id]['status'] = "fail"
        with self.parent.deliverables_building_lock:
            self.parent.deliverables_building.remove(self.id)
        if result:
            with self.parent.deliverables_open_lock:
                self.parent.deliverables[self.id]['successful_build_commit'] = self.parent.build_instruction['commit']
        return result

    def parse_d2d_filelist(self, command, thread_id):
        # currently only read from file logic
        f = open(os.path.join(command['tmp_build_target'],'filelist'), 'r')
        for line in f.readlines():
            line = line.strip()
            if '_bigfile.xml' not in line and line != "":
                self.d2d_out_dir = line
                self.path = ("%s/%s" % (self.deliverable_relative_path, line.split('/')[-1])).replace('//', '/')
        logger.debug("Deliverable build results: %s" % self.d2d_out_dir)
        command['cmd'] = command['cmd'].replace('__FILELIST__', self.d2d_out_dir)
        return command

    def extract_root_id(self, command, thread_id):
        # extract root id from DC file and then title from bigfile
        dc_path = os.path.join(self.parent.local_repo_build_dir, self.dc_file)
        with open(dc_path) as f:
            import re
            for line in f.readlines():
                #pylint: disable=W1401
                m = re.search('^\s*ROOTID\s*=\s*[\"\']?([^\"\']+)[\"\']?.*', line.strip())
                if m:
                    self.root_id = m.group(1)
                    break
        xmlstarlet = {}
        xmlstarlet['ret_val'] = 0
        if self.root_id:
            bigfile = self.root_id
            logger.debug("Found ROOTID for %s: %s" % ( self.id, self.root_id ))
            bigfile_path = (os.path.join(command['tmp_build_target'], '.tmp', '%s_bigfile.xml' % bigfile))
            xpath = "(//*[@*[local-name(.)='id']='%s']/*[contains(local-name(.),'info')]/*[local-name(.)='title']|//*[@*[local-name(.)='id']='%s']/*[local-name(.)='title'])[1]" % (
                self.root_id, self.root_id)
            xmlstarlet['cmd'] = "xmlstarlet sel -t -v \"%s\" %s" % (xpath, bigfile_path)
        else:
            bigfile = self.dc_file.replace('DC-', '')
            logger.debug("No ROOTID found for %s, using DC file name: %s" % ( self.id, self.dc_file ))
            bigfile_path = (os.path.join(command['tmp_build_target'], '.tmp', '%s_bigfile.xml' % bigfile))
            xpath = "(/*/*[contains(local-name(.),'info')]/*[local-name(.)='title']|/*/*[local-name(.)='title'])[1]"
            xmlstarlet['cmd'] = "xmlstarlet sel -t -v \"%s\" %s" % (xpath, bigfile_path)
        result = self.execute(xmlstarlet, thread_id)
        if not result:
            return False
        self.title = self.out.decode('utf-8')
        self.subdeliverable_titles = {}
        for subdeliverable in self.subdeliverables:
            xpath = "(//*[@*[local-name(.)='id']='%s']/*[contains(local-name(.),'info')]/*[local-name(.)='title']|//*[@*[local-name(.)='id']='%s']/*[local-name(.)='title'])[1]" % (
                subdeliverable, subdeliverable)
            xmlstarlet['cmd'] = "xmlstarlet sel -t -v \"%s\" %s" % (xpath, bigfile_path)
            xmlstarlet['ret_val'] = 0
            result = self.execute(xmlstarlet, thread_id)
            if not result:
                return False
            self.subdeliverable_titles[subdeliverable] = self.out.decode('utf-8')
        with self.parent.deliverables_open_lock:
            self.parent.deliverables[self.id]['title'] = self.title
            self.parent.deliverables[self.id]['path'] = self.path
        return command

    def write_deliverable_cache(self, command, thread_id):
        root = cElementTree.Element("document",
                lang=self.parent.build_instruction['lang'],
                productid=self.parent.build_instruction['product'],
                setid=self.parent.build_instruction['docset'],
                dc=self.dc_file,
                cachedate=str(time.time()))
        cElementTree.SubElement(root, "commit").text = self.parent.deliverables[self.id]['last_build_attempt_commit']
        cElementTree.SubElement(root, "path", format=self.build_format).text = self.path
        if self.root_id is not None:
            root_id = self.root_id
        else:
            root_id = ""
        cElementTree.SubElement(root, "title", rootid=root_id).text = self.title

        for subdeliverable in self.subdeliverables:
            cElementTree.SubElement(root, "title", id=subdeliverable).text = self.subdeliverable_titles[subdeliverable]
        tree = cElementTree.ElementTree(root)
        tree.write(os.path.join(self.deliverable_cache_dir, "%s.xml" % self.dc_file))
        return command

class BuildInstructionHandler:
    # This object is created when a new build request
    # is coming in via the API. This object parses the
    # incoming request and with the help of the XML
    # configuration creates a set of Deliverables.

    # A dict with meta information about a Deliverable.
    # It is filled with Deliverable.dict().
    deliverables = {}
    deliverables_lock = threading.Lock()
    # A dict that contains all Deliverable objects
    # mapped with the Deliverable ID.
    deliverable_objects = {}
    deliverable_objects_lock = threading.Lock()
    # A list of Deliverable IDs that have not yet been
    # build / have to be built.
    deliverables_open = []
    deliverables_open_lock = threading.Lock()
    # A list of Deliverable IDs that are currently
    # building. That means a worker thread is currently
    # running daps for them.
    deliverables_building = []
    deliverables_building_lock = threading.Lock()

    def __init__(self, build_instruction, config, gitLocks, gitLocksLock, thread_id):
        if self.validate(build_instruction, config):
            self.initialized = True
            self.build_instruction = build_instruction
            if 'deliverables' in build_instruction:
                self.deliverables = build_instruction['deliverables']
            self.config = config
            if not self.read_conf_dir():
                self.initialized = False
                return
            self.git_lock = RepoLock(resource_to_filename(
                self.remote_repo), thread_id, gitLocks, gitLocksLock)
            self.prepare_repo(thread_id)
            self.get_commit_hash()
        else:
            self.initialized = False
        return

    def __del__(self):
        logger.debug("Cleaning up %s" % json.dumps(self.build_instruction['id']))
        if not hasattr(self, 'local_repo_build_dir'):
            return
        cmd = shlex.split("rm -rf %s" % self.local_repo_build_dir)
        s = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE)
        s.communicate()[0]

    def __str__(self):
        return json.dumps(self.build_instruction)

    def dict(self):
        retval = self.build_instruction
        retval['open'] = self.deliverables_open
        retval['building'] = self.deliverables_building
        retval['deliverables'] = self.deliverables
        return retval

    def __getitem__(self, arg):
        return self.build_instruction

    def read_conf_dir(self):
        target = self.build_instruction['target']
        if not self.config['targets'][target]['active'] == "yes":
            logger.debug("Target %s not active." % target)
            return False
        self.stitch_tmp_dir = tempfile.mkdtemp(prefix="docserv_stitch_")
        logger.debug("Stitching XML config directory to %s" % self.stitch_tmp_dir)
        cmd = '/usr/bin/docserv-stitch --make-positive --valid-languages="%s" %s %s' % (
            self.config['server']['valid_languages'],
            self.config['targets'][target]['config_dir'],
            self.stitch_tmp_dir)
        logger.debug("Stitching command: %s" % cmd)
        cmd = shlex.split(cmd)
        s = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE)
        s.communicate()[0]
        rc = int(s.returncode)
        if rc == 0:
            logger.debug("Stitching of %s successful" %
                         self.config['targets'][target]['config_dir'])
        else:
            logger.warning("Stitching of %s failed!" %
                           self.config['targets'][target]['config_dir'])
            self.initialized = False
            return False
        # then read all files into an xml tree
        self.tree = ElementTree.parse(os.path.join(self.stitch_tmp_dir, "docserv_config_full-positive.xml"))
        xml_root = self.tree.getroot()
        try:
            xpath = ".//product[@productid='%s']/docset[@setid='%s']/builddocs/language[@lang='%s']/branch" % (
                self.build_instruction['product'], self.build_instruction['docset'], self.build_instruction['lang'])
            self.branch = xml_root.find(xpath).text

            xpath = ".//product[@productid='%s']/docset[@setid='%s']/builddocs/git/remote" % (
                self.build_instruction['product'], self.build_instruction['docset'])
            self.remote_repo = xml_root.find(xpath).text
        except AttributeError:
            logger.warning("Failed to parse xpath: %s" % xpath)
            return False
        return True

    def prepare_repo(self, thread_id):
        commands = {}
        # Clone locally cached repository, does nothing if exists
        n = 0
        local_repo_cache_dir = os.path.join(
            self.config['server']['repo_dir'], resource_to_filename(self.remote_repo))
        commands[n] = {}
        commands[n]['cmd'] = "git clone %s %s" % (
            self.remote_repo, local_repo_cache_dir)
        commands[n]['ret_val'] = None
        commands[n]['repo_lock'] = local_repo_cache_dir

        # update locally cached repo
        n += 1
        commands[n] = {}
        commands[n]['cmd'] = "git -C %s pull --all " % local_repo_cache_dir
        commands[n]['ret_val'] = 0
        commands[n]['repo_lock'] = local_repo_cache_dir

        # Create local copy in temp build dir
        n += 1
        self.local_repo_build_dir = os.path.join(self.config['server']['temp_repo_dir'], ''.join(
            random.choices(string.ascii_uppercase + string.digits, k=12)))
        commands[n] = {}
        commands[n]['cmd'] = "git clone --single-branch --branch %s %s %s" % (
            self.branch, local_repo_cache_dir, self.local_repo_build_dir)
        commands[n]['ret_val'] = 0
        commands[n]['repo_lock'] = None

        for i in range(0, n + 1):
            cmd = shlex.split(commands[i]['cmd'])
            if commands[i]['repo_lock'] is not None:
                self.git_lock.acquire()
            logger.debug("Thread %i: %s" % (thread_id, commands[i]['cmd']))
            s = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            s.communicate()
            self.git_lock.release()
            if commands[i]['ret_val'] is not None and not commands[i]['ret_val'] == int(s.returncode):
                logger.warning("Build failed! Unexpected return value %i for '%s'" % (
                    s.returncode, commands[i]['cmd']))
                self.initialized = False
                return False

        return True

    def get_commit_hash(self):
        cmd = shlex.split("git -C "+self.local_repo_build_dir +
                          " log --format=\"%H\" -n 1")
        s = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE)
        self.build_instruction['commit'] = s.communicate()[0].decode('utf-8').rstrip()
        logger.debug("Current commit hash: %s" % self.build_instruction['commit'])

    def validate(self, build_instruction, config):
        # build_instruction = {'docs': [{'docset': '15ga', 'lang': 'en', 'product': 'sles'}],'targets': ['external']}
        if not isinstance(build_instruction, dict):
            logger.warning("Validation: Is not a dict")
            return False
        if not isinstance(build_instruction['docset'], str):
            logger.warning("Validation: docset is not a string")
            return False
        if not isinstance(build_instruction['lang'], str):
            logger.warning("Validation: lang is not a string")
            return False
        if not isinstance(build_instruction['product'], str):
            logger.warning("Validation: product is not a string")
            return False
        if not isinstance(build_instruction['target'], str):
            logger.warning("Validation: target is not a string")
            return False
        logger.debug("Valid build instruction: %s" % build_instruction['id'])
        return True

    def generate_deliverables(self):
        if not self.initialized:
            return False
        xml_root = self.tree.getroot()
        logger.debug("Generating deliverables.")
        xpath = ".//product[@productid='%s']/docset[@setid='%s']/builddocs/language[@lang='%s']/deliverable" % (
            self.build_instruction['product'], self.build_instruction['docset'], self.build_instruction['lang'])
        for xml_deliverable in xml_root.findall(xpath):
            build_formats = xml_deliverable.find(".//format").attrib
            for build_format in build_formats:
                if build_formats[build_format] == "no" or build_formats[build_format] == "0" or build_formats[build_format] == "false":
                    continue
                subdeliverables = []
                for subdeliverable in xml_deliverable.findall("subdeliverable"):
                    subdeliverables.append(subdeliverable.text)
                deliverable = Deliverable(self,
                                          xml_deliverable.find(".//dc").text,
                                          build_format,
                                          subdeliverables
                                         )
                self.deliverables[deliverable.id] = deliverable.dict()
                self.deliverable_objects[deliverable.id] = deliverable
                self.deliverables_open.append(deliverable.id)
            # after all deliverables are generated, we don't need the xml tree anymore
            self.tree = None
        return True

    def get_deliverable(self):
        # return deliverable object that can run to build the output
        # return None if all deliverables are already building
        # return 'done' if all deliverables have finished building
        retval = None
        deliverable_id = None
        with self.deliverables_open_lock:
            if len(self.deliverables_open) > 0:
                deliverable_id = self.deliverables_open.pop()
                with self.deliverable_objects_lock:
                    retval = self.deliverable_objects.pop(deliverable_id)
        if retval is not None:
            with self.deliverables_building_lock:
                self.deliverables_building.append(deliverable_id)
            return retval
        with self.deliverables_building_lock:
            retval = len(self.deliverables_building)
        if retval == 0:
            return 'done'
        else:
            return None


class DocservState:
    config = {}

    ###################################################
    #   The following section contains all dicts and  #
    #   queues that keep track of stuff to be build   #
    ###################################################
    #
    # 1. "queue" for build instructions from REST API
    #
    scheduled_build_instruction = {}
    scheduled_build_instruction_lock = threading.Lock()

    #
    # 2. When an item form the scheduled_build_instruction dict
    #    is taken up for processing, its information needs
    #    to be updated from the configuration. During that phase
    #    it remains in the scheduled_build_instructions dict but
    #    its key is added to the updating_build_instruction list.
    #    This also uses the scheduled_build_instruction_lock.
    updating_build_instruction = []

    #
    # 3.1 A dict that contains all BuildInstructionHandlers.
    #     This is the state in which documents are being
    #     build.
    #
    bih_dict = {}
    bih_dict_lock = threading.Lock()

    #
    # 3.2 A queue that contains BUILD_INSTRUCTION_ID. With the ID
    #     the workers will retrieve a Deliverable and build it.
    #
    bih_queue = queue.Queue()

    #
    # 4. When a BuildInstructionHandler is finished, its
    #    status is dumped into a dict and kept for the future.
    #
    past_builds = {}
    past_builds_lock = threading.Lock()

    def __del__(self):
        pass

    def __str__(self):
        return json.dumps(self.dict())

    def dict(self):
        retval = []
        with self.scheduled_build_instruction_lock:
            for key in self.scheduled_build_instruction:
                try:
                    retval.append(self.scheduled_build_instruction[key])
                except AttributeError:
                    pass
        with self.bih_dict_lock:
            for key in self.bih_dict:
                try:
                    retval.append(self.bih_dict[key].dict())
                except AttributeError:
                    pass
        for key in self.past_builds:
            retval.append(self.past_builds[key])
        return retval

    def generate_id(self, build_instruction):
        return hashlib.md5((build_instruction['target'] +
                            build_instruction['docset'] +
                            build_instruction['lang'] +
                            build_instruction['product']).encode('utf-8')
                           ).hexdigest()[:9]

    def queue_build_instruction(self, build_instruction):
        retval = False
        build_instruction['id'] = self.generate_id(build_instruction)
        if build_instruction['id'] in self.past_builds.keys():
            with self.past_builds_lock:
                build_instruction = self.past_builds.pop(build_instruction['id'])
        with self.bih_dict_lock:
            if build_instruction['id'] not in self.bih_dict:
                with self.scheduled_build_instruction_lock:
                    if build_instruction['id'] not in self.scheduled_build_instruction:
                        self.scheduled_build_instruction[build_instruction['id']] = build_instruction
                        retval = True
        return retval

    def get_scheduled_build_instruction(self):
        # Get a build_instruction for building
        with self.scheduled_build_instruction_lock:
            for key in self.scheduled_build_instruction:
                if key not in self.updating_build_instruction:
                    self.updating_build_instruction.append(key)
                    return self.scheduled_build_instruction[key]
        return None

    def remove_scheduled_build_instruction(self, build_instruction_id):
        # Get a build_instruction for building
        with self.scheduled_build_instruction_lock:
            self.updating_build_instruction.remove(build_instruction_id)
            self.scheduled_build_instruction.pop(build_instruction_id)

    def abort_build_instruction(self, build_instruction_id):
        logger.info("Aborting build instruction %s" % build_instruction_id)
        with self.scheduled_build_instruction_lock:
            self.updating_build_instruction.remove(build_instruction_id)
            build_instruction = self.scheduled_build_instruction.pop(build_instruction_id, None)
        if build_instruction is None:
            with self.bih_dict_lock:
                build_instruction = self.bih_dict.pop(build_instruction_id).dict()
        if build_instruction is not None:
            with self.past_builds_lock:
                self.past_builds[build_instruction_id] = build_instruction

    def finish_build_instruction(self, build_instruction_id):
        logger.info("Finished build instruction %s" % build_instruction_id)
        with self.bih_dict_lock:
            build_instruction = self.bih_dict.pop(build_instruction_id)
        with self.past_builds_lock:
            self.past_builds[build_instruction_id] = build_instruction.dict()

    def get_deliverable(self, thread_id):
        try:
            build_instruction_id = self.bih_queue.get(False)
        except queue.Empty:
            return None
        else:
            deliverable = self.bih_dict[build_instruction_id].get_deliverable()
            if deliverable == 'done':
                self.finish_build_instruction(build_instruction_id)
                deliverable = None
            else: # build instruction is not yet finished, put its ID back on the queue
                self.bih_queue.put(build_instruction_id)
        return deliverable

    def save_state(self):
        f = open("/etc/docserv/%s.json" % self.config['server']['name'], "w")
        f.write(json.dumps(self.dict()))

    def load_state(self):
        logger.info("Reading previous state.")
        filepath = "/etc/docserv/%s.json" % self.config['server']['name']
        if os.path.isfile(filepath):
            file = open(filepath, "r")
            try:
                state = json.loads(file.read())
            except json.decoder.JSONDecodeError:
                return False
            for build_instruction in state:
                if len(build_instruction['building']) > 0 or len(build_instruction['open']) > 0:
                    self.queue_build_instruction(build_instruction)
                else:
                    self.past_builds[build_instruction['id']] = build_instruction
            return True
        return False


class Docserv(DocservState):
    gitLocks = {}
    gitLocksLock = threading.Lock()
    end_all = queue.Queue()

    def __init__(self, argv):
        self.parse_config(argv)
        LOGLEVELS = {0: logging.WARNING,
                     1: logging.INFO,
                     2: logging.DEBUG,
                     }
        logger.setLevel(LOGLEVELS[self.config['server']['loglevel']])
        self.load_state()

    def parse_config(self, argv):
        config = configparser.ConfigParser()
        if len(argv) == 1:
            config_file = "docserv"
        else:
            config_file = argv[1]
        logger.info("Reading /etc/docserv/%s.ini" % config_file)
        config.read("/etc/docserv/%s.ini" % config_file)

        try:
            self.config['server'] = {}
            self.config['server']['name'] =             config_file
            self.config['server']['loglevel'] =     int(config['server']['loglevel'])
            self.config['server']['host'] =             config['server']['host']
            self.config['server']['port'] =         int(config['server']['port'])
            self.config['server']['repo_dir'] =         config['server']['repo_dir']
            self.config['server']['temp_repo_dir'] =    config['server']['temp_repo_dir']
            self.config['server']['valid_languages'] =  config['server']['valid_languages']
            self.config['server']['max_threads'] =  int(config['server']['max_threads'])
            self.config['targets'] = {}
            for section in config.sections():
                if not str(section).startswith("target_"):
                    continue
                self.config['targets'][config[section]['name']] =                  {}
                self.config['targets'][config[section]['name']]['name'] =          config[section]['name']
                self.config['targets'][config[section]['name']]['template_dir'] = config[section]['template_dir']
                self.config['targets'][config[section]['name']]['active'] =        config[section]['active']
                self.config['targets'][config[section]['name']]['draft'] =         config[section]['draft']
                self.config['targets'][config[section]['name']]['remarks'] =       config[section]['remarks']
                self.config['targets'][config[section]['name']]['meta'] =          config[section]['meta']
                self.config['targets'][config[section]['name']]['beta_warning'] =  config[section]['beta_warning']
                self.config['targets'][config[section]['name']]['target_path'] =   config[section]['target_path']
                self.config['targets'][config[section]['name']]['backup_path'] =   config[section]['backup_path']
                self.config['targets'][config[section]['name']]['config_dir'] =    config[section]['config_dir']
                self.config['targets'][config[section]['name']]['languages'] =     config[section]['languages']
                self.config['targets'][config[section]['name']]['default_lang'] =  config[section]['default_lang']
                self.config['targets'][config[section]['name']]['internal'] =      config[section]['internal']
        except KeyError as error:
            logger.warning("Invalid configuration file, missing configuration key '%s'. Exiting." % error)
            sys.exit(1)

    def start(self):
        # start everything
        try:
            thread_receive = threading.Thread(target=self.listen)
            thread_receive.start()
            workers = []
            for i in range(0, min([os.cpu_count(), self.config['server']['max_threads']])):
                logger.info("Starting build thread %i" % i)
                worker = threading.Thread(target=self.worker, args=(i,))
                worker.start()
                workers.append(worker)
            # to have a clean shutdown, wait for all threads to finish
            thread_receive.join()
        except KeyboardInterrupt:
            self.exit()
        for worker in workers:
            worker.join()
        self.rest.shutdown()
        self.save_state()

    def exit(self):
        logger.warning("Received SIGINT. Telling all threads to end. Please wait.")
        self.end_all.put("now")

    def parse_build_instruction(self, thread_id):
        build_instruction = self.get_scheduled_build_instruction()
        if build_instruction is not None:
            myBIH = BuildInstructionHandler(build_instruction, self.config, self.gitLocks, self.gitLocksLock, thread_id)
            # If the initialization failed, immediately delete the BuildInstructionHandler
            if myBIH.initialized == False:
                self.abort_build_instruction(build_instruction['id'])
                return
            myBIH.generate_deliverables()
            self.remove_scheduled_build_instruction(build_instruction['id'])
            with self.bih_dict_lock:
                self.bih_dict[build_instruction['id']] = myBIH
            self.bih_queue.put(build_instruction['id'])

    def worker(self, thread_id):
        while(True):
            # 1. parse input from rest api and put the instance of the doc class on the currently building queue
            self.parse_build_instruction(thread_id)

            # 2. get doc from currently_building queue and then a deliverable from doc.
            #    after that, put doc back on the currently building queue. unless it was
            #    the last deliverable.
            deliverable = self.get_deliverable(thread_id)
            if deliverable is not None:
                deliverable.run(thread_id)

            # 3. end thread if sigint
            if not self.end_all.empty(): return True

            # 4. wait for a short while, then repeat
            time.sleep(0.1)

            # Thread 0 is frequently saving the state
            if thread_id == 0:
                self.save_state()

    def listen(self):
        server_address = (self.config['server']['host'], int(
            self.config['server']['port']))
        self.rest = ThreadedRESTServer(server_address, RESTServer, self)
        self.rest.serve_forever()
        return True


class RESTServer(BaseHTTPRequestHandler):
    def _set_headers(self):
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.end_headers()

    def do_GET(self):
        if self.path == '/' or self.path == '/build_instructions/':
            self._set_headers()
            self.wfile.write(
                bytes(json.dumps(self.server.docserv.dict()), "utf-8"))
        elif self.path == '/deliverables/':
            self._set_headers()
            self.wfile.write(
                bytes(json.dumps(self.server.docserv.deliverables), "utf-8"))

    def do_POST(self):
        content_length = int(self.headers['Content-Length'])
        # [{"docset": "15ga", "lang": "en-us", "product": "sles", "target": "external"}. ]
        post_data = self.rfile.read(content_length)
        build_jobs = json.loads(post_data)
        for job in build_jobs:
            if self.server.docserv.queue_build_instruction(job):
                logger.info("Queueing %s" % json.dumps(job))
            else:
                logger.info("Not queueing %s" % json.dumps(job))
        self._set_headers()

class ThreadedRESTServer(ThreadingMixIn, HTTPServer):
    def __init__(self, server_address, RequestHandlerClass, docserv, bind_and_activate=True):
        HTTPServer.__init__(self, server_address,
                            RequestHandlerClass, bind_and_activate)
        logger.info("Starting HTTP server on %s:%i" % server_address)
        self.docserv = docserv

def print_help():
    print("""This is a deamon and should be invoked with the command:

> systemctl start docserv@CONFIG_FILENAME
or
> docserv CONFIG_FILENAME

The CONFIG_FILENAME must reside in /etc/docserv/ and end with .ini.""")

logger = logging.getLogger('docserv')
logger.setLevel(logging.INFO)

ch = logging.StreamHandler(sys.stdout)
ch.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
ch.setFormatter(formatter)
logger.addHandler(ch)

if __name__ == "__main__":
    if "--help" in sys.argv or "-h" in sys.argv:
        print_help()
    else:
        docserv = Docserv(sys.argv)
        docserv.start()
        sys.exit(0)
